{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tao\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "from transformers import DebertaForSequenceClassification, DebertaTokenizer, Trainer, TrainingArguments\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just making sure that the model generalises well to the dev set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Tao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>White House press secretary Sean Spicer said t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\" Just like we received migrants fleeing El Sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>Sri Lankan norms and culture inhibit women fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10465</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\" She has one huge platform , and information ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\" Anja Ringgren Loven I ca n't find a word to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\" Guinness World Record of 540lbs of 7-layer m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       par_id      art_id     keyword country  \\\n",
       "0           1  @@24942188    hopeless      ph   \n",
       "1           2  @@21968160     migrant      gh   \n",
       "2           3  @@16584954   immigrant      ie   \n",
       "3           4   @@7811231    disabled      nz   \n",
       "4           5   @@1494111     refugee      ca   \n",
       "...       ...         ...         ...     ...   \n",
       "10464   10465  @@14297363       women      lk   \n",
       "10465   10466  @@70091353  vulnerable      ph   \n",
       "10466   10467  @@20282330     in-need      ng   \n",
       "10467   10468  @@16753236    hopeless      in   \n",
       "10468   10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label  \n",
       "0      We 're living in times of absolute insanity , ...      0  \n",
       "1      In Libya today , there are countless number of...      0  \n",
       "2      White House press secretary Sean Spicer said t...      0  \n",
       "3      Council customers only signs would be displaye...      0  \n",
       "4      \" Just like we received migrants fleeing El Sa...      0  \n",
       "...                                                  ...    ...  \n",
       "10464  Sri Lankan norms and culture inhibit women fro...      0  \n",
       "10465  He added that the AFP will continue to bank on...      0  \n",
       "10466  \" She has one huge platform , and information ...      1  \n",
       "10467  \" Anja Ringgren Loven I ca n't find a word to ...      1  \n",
       "10468  \" Guinness World Record of 540lbs of 7-layer m...      1  \n",
       "\n",
       "[10469 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if cuda_available else torch.device('cpu')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "data = pd.read_csv(\"../data/dontpatronizeme_pcl.tsv\",\n",
    "                       sep=\"\\t\",\n",
    "                       names=['par_id', 'art_id', 'keyword', 'country', 'text', 'label'],\n",
    "                       skiprows=4)\n",
    "\n",
    "data['label'] = data['label'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
    "\n",
    "trids = pd.read_csv('../data/train_semeval_parids-labels.csv')\n",
    "teids = pd.read_csv('../data/dev_semeval_parids-labels.csv')\n",
    "\n",
    "trids.par_id = trids.par_id\n",
    "teids.par_id = teids.par_id\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>The 25-minute-long game of heated taunts was i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>It is seen in recurring violence and continuin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>650</td>\n",
       "      <td>in-need</td>\n",
       "      <td>When contacted , Yadav said , \" There are two ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9046</td>\n",
       "      <td>migrant</td>\n",
       "      <td>Even so , many speakers figured out at various...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9950</td>\n",
       "      <td>homeless</td>\n",
       "      <td>In June , several NGOs decided to reduce food ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>10309</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>Reading Future Sex it turns out that my friend...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>9186</td>\n",
       "      <td>women</td>\n",
       "      <td>One bright spot in all of this is that while t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>9254</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>My colleague and friend is the multi-lingual d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>10252</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>Pressure group Volta4Change is set to march ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>8930</td>\n",
       "      <td>migrant</td>\n",
       "      <td>In June this year , the government said that t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id  community                                               text  \\\n",
       "0       9781  immigrant  The 25-minute-long game of heated taunts was i...   \n",
       "1      10007   hopeless  It is seen in recurring violence and continuin...   \n",
       "2        650    in-need  When contacted , Yadav said , \" There are two ...   \n",
       "3       9046    migrant  Even so , many speakers figured out at various...   \n",
       "4       9950   homeless  In June , several NGOs decided to reduce food ...   \n",
       "...      ...        ...                                                ...   \n",
       "2089   10309   hopeless  Reading Future Sex it turns out that my friend...   \n",
       "2090    9186      women  One bright spot in all of this is that while t...   \n",
       "2091    9254  immigrant  My colleague and friend is the multi-lingual d...   \n",
       "2092   10252   hopeless  Pressure group Volta4Change is set to march ag...   \n",
       "2093    8930    migrant  In June this year , the government said that t...   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         1  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "2089      0  \n",
       "2090      0  \n",
       "2091      0  \n",
       "2092      0  \n",
       "2093      0  \n",
       "\n",
       "[2093 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild test set \n",
    "\n",
    "rows = [] # will contain par_id, label and text\n",
    "for idx in range(len(teids)):\n",
    "  parid = teids.par_id[idx]\n",
    "  #print(parid)\n",
    "  # select row from original dataset\n",
    "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
    "  text = data.loc[data.par_id == parid].text.values[0]\n",
    "  label = data.loc[data.par_id == parid].label.values[0]\n",
    "  rows.append({\n",
    "      'par_id':parid,\n",
    "      'community':keyword,\n",
    "      'text':text,\n",
    "      'label':label\n",
    "  })\n",
    "  \n",
    "test_set = pd.DataFrame(rows)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in test_set.iterrows():\n",
    "    if type(row['text']) is not str:   \n",
    "        test_set.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "test_encodings = tokenizer(list(test_set[\"text\"]), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "test_labels = list(test_set[\"label\"])\n",
    "test_dataset = Dataset(test_encodings, test_labels)\n",
    "\n",
    "def compute_metrics(input):\n",
    "    y_pred = np.argmax(input.predictions, axis=1)\n",
    "    y_true = input.label_ids\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1score = f1_score(y_true, y_pred)\n",
    "    return {'accuracy': accuracy, 'f1 score': f1score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DebertaForSequenceClassification.from_pretrained(\"../models/deberta_upsample/deberta_upsample\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tao\\AppData\\Local\\Temp\\ipykernel_20508\\4024080124.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3c0e0601704b0ba25184a2923962c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6886156797409058,\n",
       " 'eval_accuracy': 0.9283325370281892,\n",
       " 'eval_f1 score': 0.5833333333333334,\n",
       " 'eval_runtime': 22.8183,\n",
       " 'eval_samples_per_second': 91.725,\n",
       " 'eval_steps_per_second': 11.482}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Deberta/\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.05,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1 score\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
